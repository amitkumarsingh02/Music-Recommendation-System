# -*- coding: utf-8 -*-
"""Algorithm - Using Album, Artist, Genre [1-21] Ratings

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/155amDMQnrR5a4ua67Lbn8wDKZMZZQTbd
"""

"""### Method 3 : Various Methods"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import lit
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler   
from pyspark.ml.evaluation import BinaryClassificationEvaluator
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")

spark = SparkSession.builder.appName('Various_Methods').getOrCreate()
data_predicted = pd.read_csv("./data/test2_new.txt", sep='|', names=['userID', 'trackID', 'predictions'])

genre_columns = ['genre1', 'genre2', 'genre3', 'genre4', 'genre5', \
                  'genre6', 'genre7', 'genre8', 'genre9', 'genre10', \
                  'genre11', 'genre12', 'genre13', 'genre14', 'genre15', \
                  'genre16', 'genre17', 'genre18', 'genre19', 'genre20', \
                  'genre21']

output_df = pd.read_csv("./data/output21.txt", sep='|', names=['userID', 'trackID', 'album', 'artist', *genre_columns])

stats_columns = ['album', 'artist']
stats = output_df[genre_columns].apply(pd.DataFrame.describe, axis=1)
output_df['genre_max'], output_df['genre_min'], output_df['genre_mean'] = stats['max'], stats['min'], stats['mean']
output_df['genre_variance'] = stats['std']**2
output_df['weighted_score'] = output_df['album'] * 0.85 + output_df['artist'] * 0.15 + output_df['genre_mean'] * 0.05
#selectedFeatures = ['userID', 'trackID', 'album', 'artist', 'genre_max', 'genre_min', 'genre_mean', 'genre_variance', 'weighted_score']
selectedFeatures = ['userID', 'trackID', 'album', 'artist', 'genre_mean', 'weighted_score'] 
output_df = output_df[selectedFeatures].copy()
all_data = data_predicted.merge(output_df, on=['userID', 'trackID']).fillna(0) 
all_data.columns[all_data.isnull().any()] # check for null values

getFeature = selectedFeatures + ['predictions']
all_data = all_data[selectedFeatures + ['predictions']].copy()
all_data = all_data.sort_values(by='userID')
all_data.to_csv('allData.csv', index=False)
all_data.count()
predicted_df = spark.read.csv('allData.csv', header=True, inferSchema=True)
numericCols = ['album', 'artist', 'genre_mean', 'weighted_score']
stages = []
assemblerInputs =  numericCols
assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features")
stages += [assembler]
label_stringIdx = StringIndexer(inputCol = 'predictions', outputCol = 'label')
stages += [label_stringIdx]
predicted_df.toPandas().dtypes

from pyspark.ml import Pipeline
pipeline = Pipeline(stages = stages)
pipelineModel = pipeline.fit(predicted_df)
df = pipelineModel.transform(predicted_df)
cols = predicted_df.columns
selectedCols = ['label', 'features'] + cols
df = df.select(selectedCols)
df.printSchema()

pd.DataFrame(df.take(5), columns=df.columns).transpose()

output_df.to_csv('output.csv', index=False)
all_data_df = spark.read.csv('output.csv', header=True, inferSchema=True)
all_data_df.printSchema()
main_cols = all_data_df.columns
all_data_df = pipelineModel.transform(all_data_df)
selCols = ['features'] + main_cols
all_data_df = all_data_df.select(selCols)
all_data_df.printSchema()

train, test = df.randomSplit([0.8, 0.2], seed = 2018)
print("Training Dataset Count: " + str(train.count()))
print("Test Dataset Count: " + str(test.count()))

numericCols = ['album_score', 'artist_score']
stages = []
assemblerInputs =  numericCols
assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features")
stages += [assembler]

label_stringIdx = StringIndexer(inputCol = 'prediction', outputCol = 'label')
stages += [label_stringIdx]

"""#### 3.1. Logistic Regression Model"""

from pyspark.ml.classification import LogisticRegression
logistic_regression = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=100)
logistic_regression_model = logistic_regression.fit(train)

trainingSummary = logistic_regression_model.summary
roc = trainingSummary.roc.toPandas()
print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))

predictions = logistic_regression_model.transform(test)
predictions.select('userID', 'trackID', 'label', 'probability', 
                   'rawPrediction', 'prediction' ).show(12)

from pyspark.sql.functions import col
sort_predictions = predictions.select('userID', 'trackID', 
                                      'label', 'probability', 
                                      'rawPrediction', 'prediction' 
                                     ).sort(col("userID").asc(), col("probability").desc())

logistic_regression_model = logistic_regression.fit(df) # training on full dataset
predictions_lr = logistic_regression_model.transform(all_data_df)
logistic_regression_pred = predictions_lr.select('UserID','trackID','prediction', 'weighted_score')
logistic_regression_pred = logistic_regression_pred.toPandas()

logistic_regression_pred = logistic_regression_pred.sort_values(by = ['UserID', 'prediction', 'weighted_score'], ascending = [True, False, False])
users = logistic_regression_pred['UserID'].unique()
temp_df = pd.DataFrame()
for userId in users:
    frame_to_update = logistic_regression_pred.loc[logistic_regression_pred['UserID'] == userId]
    frame_to_update.head(3)['prediction'] = 1.0
    frame_to_update.tail(3)['prediction'] = 0.0
    temp_df = temp_df.append(frame_to_update, ignore_index = True)
logistic_regression_pred = temp_df

logistic_regression_pred['TrackID'] = logistic_regression_pred['UserID'].astype(str) + '_' + logistic_regression_pred['trackID'].astype(str)
logistic_regression_pred = logistic_regression_pred.rename(columns={'prediction':'Predictor'})
logistic_regression_pred.drop(columns={'UserID', 'trackID', 'weighted_score'}, inplace=True)
logistic_regression_pred.to_csv('./kaggle_submissions/logistic_regression_pred.csv', index=False)

"""#####  3.2. Decision Tree Classifier"""

from pyspark.ml.classification import DecisionTreeClassifier
decision_tree_classifier = DecisionTreeClassifier(featuresCol='features', labelCol='label', maxDepth=3)
decision_tree_classifier_model = decision_tree_classifier.fit(train)
predictions_decision_tree_classifier = decision_tree_classifier_model.transform(test)

evaluator = BinaryClassificationEvaluator()
print("Test Area Under ROC: " + str(evaluator.evaluate(predictions_decision_tree_classifier, {evaluator.metricName: "areaUnderROC"})))

sort_predictions_decision_tree_classifier = predictions_decision_tree_classifier.select('userID', 'trackID', 'label', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_predictions_decision_tree_classifier.show(6)

decision_tree_classifier_model = decision_tree_classifier.fit(df) # training on full dataset
predictions_dt = decision_tree_classifier_model.transform(all_data_df)
decision_tree_pred = predictions_dt.select('UserID','trackID','prediction', 'weighted_score')
decision_tree_pred = decision_tree_pred.toPandas()

decision_tree_pred = decision_tree_pred.sort_values(by = ['UserID', 'prediction', 'weighted_score'], ascending = [True, False, False])
users = decision_tree_pred['UserID'].unique()
temp_df = pd.DataFrame()
for userId in users:
    frame_to_update = decision_tree_pred.loc[decision_tree_pred['UserID'] == userId]
    frame_to_update.head(3)['prediction'] = 1.0
    frame_to_update.tail(3)['prediction'] = 0.0
    temp_df = temp_df.append(frame_to_update, ignore_index = True)
decision_tree_pred = temp_df

decision_tree_pred['TrackID'] = decision_tree_pred['UserID'].astype(str) + '_' + decision_tree_pred['trackID'].astype(str)
decision_tree_pred = decision_tree_pred.rename(columns={'prediction':'Predictor'})
decision_tree_pred.drop(columns={'UserID', 'trackID', 'weighted_score'}, inplace=True)

"""#####  3.3. Random Forest Classifier"""

from pyspark.ml.classification import RandomForestClassifier
random_forest = RandomForestClassifier(featuresCol='features', labelCol='label')
random_forest_model = random_forest.fit(train)
random_forest_predictions = random_forest_model.transform(test)

evaluator = BinaryClassificationEvaluator()
print("Test Area Under ROC: " + str(evaluator.evaluate(random_forest_predictions, {evaluator.metricName: "areaUnderROC"})))
sort_random_forest_predictions = random_forest_predictions.select('userID', 'trackID', 'label', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())

random_forest_model = random_forest.fit(df) # training on full dataset
predictions_rt = random_forest_model.transform(all_data_df)
random_forest_pred = predictions_rt.select('UserID','trackID','prediction', 'weighted_score')
random_forest_pred = random_forest_pred.toPandas()

random_forest_pred = random_forest_pred.sort_values(by = ['UserID', 'prediction', 'weighted_score'], ascending = [True, False, False])
users = random_forest_pred['UserID'].unique()
temp_df = pd.DataFrame()
for userId in users:
    frame_to_update = random_forest_pred.loc[random_forest_pred['UserID'] == userId]
    frame_to_update.head(3)['prediction'] = 1.0
    frame_to_update.tail(3)['prediction'] = 0.0
    temp_df = temp_df.append(frame_to_update, ignore_index = True)
random_forest_pred = temp_df

random_forest_pred['TrackID'] = random_forest_pred['UserID'].astype(str) + '_' + random_forest_pred['trackID'].astype(str)
random_forest_pred = random_forest_pred.rename(columns={'prediction':'Predictor'})
random_forest_pred.drop(columns={'UserID', 'trackID', 'weighted_score'}, inplace=True)
random_forest_pred.to_csv('./kaggle_submissions/random_forest_pred.csv', index=False)

"""##### 6.4. Gradient Boosted Tree Classifier"""

from pyspark.ml.classification import GBTClassifier
gbt_classifier = GBTClassifier(maxIter=100)
gbt_classifier_model = gbt_classifier.fit(train)
gradient_boosted_tree_classifier_predictions = gbt_classifier_model.transform(test)

evaluator = BinaryClassificationEvaluator()
print("Test Area Under ROC: " + str(evaluator.evaluate(gradient_boosted_tree_classifier_predictions, {evaluator.metricName: "areaUnderROC"})))

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
paramGrid = (ParamGridBuilder().addGrid(gbt_classifier.maxDepth, [2, 4, 6])
.addGrid(gbt_classifier.maxBins, [20, 60])
.addGrid(gbt_classifier.maxIter, [10, 20])
.build())
cv = CrossValidator(estimator=gbt_classifier, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)
cvModel = cv.fit(train)
predictions = cvModel.transform(test)
evaluator.evaluate(predictions)

sort_gradient_boosted_tree_classifier_predictions = gradient_boosted_tree_classifier_predictions.select('userID', 'trackID', 'label', 'probability', 'rawPrediction', 'prediction').sort(col('userID').asc(), col('probability').desc())
sort_gradient_boosted_tree_classifier_predictions.show(6)

gradient_boosted_tree_classifier_model = cv.fit(df) # training on full dataset
predictions_gb = gradient_boosted_tree_classifier_model.transform(all_data_df)
gbc_pred = predictions_gb.select('UserID','trackID','prediction', 'weighted_score')
gbc_pred = gbc_pred.toPandas()

gbc_pred = gbc_pred.sort_values(by = ['UserID', 'prediction', 'weighted_score'], ascending = [True, False, False])
users = gbc_pred['UserID'].unique()
temp_df = pd.DataFrame()
for userId in users:
    frame_to_update = gbc_pred.loc[gbc_pred['UserID'] == userId]
    frame_to_update.head(3)['prediction'] = 1.0
    frame_to_update.tail(3)['prediction'] = 0.0
    temp_df = temp_df.append(frame_to_update, ignore_index = True)
gbc_pred = temp_df

gbc_pred['TrackID'] = gbc_pred['UserID'].astype(str) + '_' + gbc_pred['trackID'].astype(str)
gbc_pred = gbc_pred.rename(columns={'prediction':'Predictor'})
gbc_pred.drop(columns={'UserID', 'trackID', 'weighted_score'}, inplace=True)
gbc_pred.to_csv('./kaggle_submissions/gbc_pred.csv', index=False)



